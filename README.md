# Markov Language Generator
Inspired by [this](https://www.cs.princeton.edu/courses/archive/spr05/cos126/assignments/markov.html) archived Princeton comp sci assignment and [this](http://www.bowdoin.edu/~sharmon/courses/3725/fall20/labs/m2_markov-chains/) Bowdoin lab!

I stumbled across Markov chains while reading through old quiz bowl tossups. Wikipedia defines a Markov chain as a "stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event." 

Without all the jargon: Markov chains model movement between different **states** based on the **probability** of transitioning from any given state to another. A pure, first-order Markov chain is **"memoryless"** in that this probability depends only on the current state. ***n*-order Markov chains take the previous *n*-states into account** (e.g., a second-order chain considers both the current state and the one preceding it). 

A **transition matrix** is generated by iterating through the input data, logging each state (or sequence of states in a higher-order chain), and calculating the respective probabilities of every other state succeeding it. Using this transition matrix, one can generate a Markov chain by traversing the probability distribution of the last state (or *n*-states) to select the next state. 

In this program, each **word** (or sequence of words of length *n*) of the input text is considered to be a **state**. A **transition matrix** and **strings of text** are generated as described above; the user may set the order of the Markov chain, the number of strings to output, and the number of words per string. **The higher the order, the more closely the output text will resemble the input.** A **second-order** chain seems to be best for generating text closely modeled on, but not identical to the input. Keep in mind that the **transition matrices of longer input texts will have more diverse probability distributions for each state** (and therefore generate more interesting and original output text), but *this comes at the expense of CPU and performance time*. 

## Fun Ideas:
- Invent new tunes by copy/pasting [every Beatles song title](https://songs-tube.net/artist.php?id=16282video-links/) and generating short strings (3-5 words) with a low Markov order (1 or 2).
- Copy/paste a full novel from Project Gutenberg (ex. [Uncle Tom's Cabin](https://www.gutenberg.org/cache/epub/11171/pg11171.txt)) and generate 250-word paragraphs with a Markov order of 3.
- Copy/paste at least 3 articles about COVID-19 and generate a 500-word article if you just can't get enough of the news.

It's really not as complicated as it sounds--copy/paste *something* and have fun!
