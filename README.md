# Markov Language Generator
Inspired by [this](https://www.cs.princeton.edu/courses/archive/spr05/cos126/assignments/markov.html) archived Princeton comp sci assignment and [this](http://www.bowdoin.edu/~sharmon/courses/3725/fall20/labs/m2_markov-chains/) Bowdoin lab. 

I stumbled across Markov chains while reading through old quiz bowl tossups. Wikipedia defines a Markov chain as a "stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event." 

Without all the jargon: Markov chains model movement between different **states** based on the **probability** of transitioning from any given state to another. A pure, first-order Markov chain is **"memoryless" **in that this probability depends only on the current state. ** *n*-order Markov chains take the previous *n*-states into account** (e.g., a second-order chain considers both the current state and the one preceding it). 

A **transition matrix** is generated by iterating through the input data, logging each state (or sequence of states in a higher-order chain), and calculating the respective probabilities of every other state succeeding it. Using this transition matrix, one can generate a Markov chain by traversing the probability distribution of the last *n*-states to select the next state. 

In this program, each **word** (or sequence of words of length *n*) of the **input text** is considered to be a state. A transition matrix and strings of text are generated as described above; the user may set the order of the Markov chain, the number of strings to output, and the number of words per string. The higher the order, the more closely the output text will resemble the input (and the longer the matrix generation will take!) A second-order chain seems to be best for generating text closely modeled on, but not identical to the input. Keep in mind that the transition matrices of longer input texts will have more diverse probability distributions for each state (and therefore generate more interesting and original output text), but this comes at the expense of CPU and performance time. 

It's really not as complicated as it sounds--copy/paste a news article/speech/book or something and have fun!
