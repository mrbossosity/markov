# Markov Language Generator
Inspired by [this](https://www.cs.princeton.edu/courses/archive/spr05/cos126/assignments/markov.html) archived Princeton comp sci assignment and [this](http://www.bowdoin.edu/~sharmon/courses/3725/fall20/labs/m2_markov-chains/) Bowdoin lab. 

I stumbled across Markov chains while reading through old quiz bowl tossups. Wikipedia defines a Markov chain as a "stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event." 

Without all the jargon: Markov chains model movement between different **states** based on the **probability** of transitioning from any given state to another. A pure, first-order Markov chain is **"memoryless"** in that this probability depends only on the current state. ***n*-order Markov chains take the previous *n*-states into account** (e.g., a second-order chain considers both the current state and the one preceding it). 

A **transition matrix** is generated by iterating through the input data, logging each state (or sequence of states in a higher-order chain), and calculating the respective probabilities of every other state succeeding it. Using this transition matrix, one can generate a Markov chain by traversing the probability distribution of the last *n*-states to select the next state. 

In this program, each **word** (or sequence of words of length *n*) of the **input text** is considered to be a state. A transition matrix and strings of text are generated as described above; the user may set the order of the Markov chain, the number of strings to output, and the number of words per string. **The higher the order, the more closely the output text will resemble the input (and the longer the matrix generation will take!)** A **second-order** chain seems to be best for generating text closely modeled on, but not identical to the input. Keep in mind that the **transition matrices of longer input texts will have more diverse probability distributions for each state (and therefore generate more interesting and original output text)**, but this comes at the expense of **CPU and performance time**. 

It's really not as complicated as it sounds--copy/paste a news article/speech/book or something and have fun!
